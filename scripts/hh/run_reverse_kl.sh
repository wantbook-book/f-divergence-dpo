# 12:57pm, June 26, 2023
######## training for dpo using reverse_kl but sft is done on hh and shp. ########
ulimit -n 64000; python -u train.py model=pythia28 datasets=[hh] loss=dpo fsdp_port=10888 lr=1e-6 loss.alpha=1.0 loss.divergence=reverse_kl loss.beta=0.1 exp_name=anthropic_dpo_reverse_kl_pythia28_hh0.1 gradient_accumulation_steps=2 batch_size=64 eval_batch_size=32 trainer=FSDPTrainer sample_during_eval=false model.fsdp_policy_mp=bfloat16 model.archive=/pubshare/fwk/dpo_cache/jovyan/anthropic_sft_pythia28_2024-08-20_08-13-01_003749/LATEST/policy.pt